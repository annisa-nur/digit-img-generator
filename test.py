# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tkw2vbmK43zbzLwNoQaSR1NdWpLi2uex
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt

import streamlit as st
from tensorflow.keras.models import load_model
import cv2

# Load the MNIST dataset
(x_train, y_train), (_, _) = mnist.load_data()
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255

# Function to preprocess the image (same as the training preprocessing)
def preprocess_image(img):
    # 1. Invert the image: Change black background to white and digit to black
    inverted_img = cv2.bitwise_not(img)

    # 2. Rescale and convert the image to uint8
    inverted_img = (inverted_img * 255).astype(np.uint8)

    # 3. Find contours to capture the biggest contour (digit)
    _, thresh = cv2.threshold(inverted_img, 127, 255, cv2.THRESH_BINARY_INV)
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # 4. Get the largest contour (this corresponds to the digit)
    largest_contour = max(contours, key=cv2.contourArea)

    # 5. Get bounding box for the largest contour
    x, y, w, h = cv2.boundingRect(largest_contour)

    # 6. Crop the digit and place it in the center of a new 28x28 canvas
    cropped_digit = inverted_img[y:y+h, x:x+w]

    # Create a new blank canvas of 28x28 pixels (white background)
    centered_digit = np.ones((28, 28), dtype=np.uint8) * 255

    # Calculate the offset to center the digit
    offset_x = (28 - cropped_digit.shape[1]) // 2
    offset_y = (28 - cropped_digit.shape[0]) // 2

    # Place the cropped digit in the center of the 28x28 canvas
    centered_digit[offset_y:offset_y+cropped_digit.shape[0], offset_x:offset_x+cropped_digit.shape[1]] = cropped_digit

    return centered_digit

# Apply preprocessing to the MNIST dataset
x_train_preprocessed = np.array([preprocess_image(img) for img in x_train])

# Reshape the x_train_preprocessed to include the channels dimension
x_train_preprocessed = x_train_preprocessed.reshape(x_train_preprocessed.shape[0], 28, 28, 1)

# Define the data augmentation strategy
datagen = ImageDataGenerator(
    rotation_range=20,       # Random rotation
    width_shift_range=0.2,   # Horizontal shifts
    height_shift_range=0.2,  # Vertical shifts
    zoom_range=0.2,          # Random zoom
    shear_range=0.2,         # Random shearing
    horizontal_flip=True,    # Random horizontal flip
    fill_mode='nearest'      # Fill missing pixels with the nearest value
)

# Fit the data generator on the preprocessed data
datagen.fit(x_train_preprocessed)

# Now, you can train your GAN using the augmented and preprocessed data

# Build the Generator model
def build_generator():
    model = models.Sequential([
        layers.Dense(128, input_dim=100),
        layers.LeakyReLU(alpha=0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(256),
        layers.LeakyReLU(alpha=0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(512),
        layers.LeakyReLU(alpha=0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(1024),
        layers.LeakyReLU(alpha=0.2),
        layers.BatchNormalization(momentum=0.8),
        layers.Dense(28 * 28 * 1, activation='tanh'),
        layers.Reshape((28, 28, 1))
    ])
    return model

# Build the Discriminator model
def build_discriminator():
    model = models.Sequential([
        layers.Flatten(input_shape=(28, 28, 1)),
        layers.Dense(512),
        layers.LeakyReLU(alpha=0.2),
        layers.Dropout(0.4),
        layers.Dense(256),
        layers.LeakyReLU(alpha=0.2),
        layers.Dropout(0.4),
        layers.Dense(1, activation='sigmoid')
    ])
    return model

# Build the GAN model (stack the generator and discriminator)
def build_gan(generator, discriminator):
    discriminator.trainable = False
    model = models.Sequential([generator, discriminator])
    return model

# Compile the models
generator = build_generator()
discriminator = build_discriminator()
gan = build_gan(generator, discriminator)

discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
gan.compile(optimizer='adam', loss='binary_crossentropy')

# Training function
def train_gan(epochs=100, batch_size=64, save_interval=1000):
    half_batch = batch_size // 2
    for epoch in range(epochs):
        # Train the discriminator
        idx = np.random.randint(0, x_train_preprocessed.shape[0], half_batch)
        real_imgs = x_train_preprocessed[idx]
        noise = np.random.normal(0, 1, (half_batch, 100))
        fake_imgs = generator.predict(noise)

        # Train the discriminator (real images = 1, fake images = 0)
        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))
        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((half_batch, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train the generator (try to fool the discriminator)
        noise = np.random.normal(0, 1, (batch_size, 100))
        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

        # Print the progress
        if epoch % save_interval == 0:
            print(f"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]")

            # Save generated image samples
            if epoch % (save_interval * 10) == 0:
                save_generated_images(epoch)

            # Save models (both generator and discriminator) after each save interval
            save_models(epoch)

# Save generated images to visualize progress
def save_generated_images(epoch, examples=5, noise_dim=100, img_shape=(28, 28, 1)):
    noise = np.random.normal(0, 1, (examples, noise_dim))
    generated_images = generator.predict(noise)
    generated_images = (0.5 * generated_images + 0.5) * 255.0
    for i in range(examples):
        plt.subplot(1, examples, i + 1)
        plt.imshow(generated_images[i], interpolation='nearest', cmap='gray')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig(f"generated_images_epoch_{epoch}.png")
    plt.close()

# Save the generator and discriminator models
def save_models(epoch):
    generator.save(f"generator_epoch_{epoch}.h5")
    discriminator.save(f"discriminator_epoch_{epoch}.h5")
    print(f"Models saved at epoch {epoch}")

# Train the GAN
train_gan(epochs=100, batch_size=64, save_interval=1000)

# Load the pre-trained generator model
generator = load_model('generator_epoch_0.h5')

# Streamlit UI for digit input
st.title("Generate Handwritten Digit Images using GAN")

# User input to choose a digit
digit = st.number_input("Enter a digit (0-9):", min_value=0, max_value=9)

# Function to generate images using the GAN model
def generate_digit_image(digit, num_images=5):
    noise = np.random.normal(0, 1, (num_images, 100))  # Latent space noise vector
    generated_images = generator.predict(noise)

    # Rescale images to 0-255 for display
    generated_images = (0.5 * generated_images + 0.5) * 255.0

    return generated_images

if st.button("Generate Images"):
    # Generate 5 images of the specified digit
    generated_images = generate_digit_image(digit)

    # Plot the images
    fig, axes = plt.subplots(1, 5, figsize=(15, 15))
    for i, ax in enumerate(axes):
        ax.imshow(generated_images[i].reshape(28, 28), cmap='gray')
        ax.axis('off')
    st.pyplot(fig)

